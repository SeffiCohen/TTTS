{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Analysis of Random Forest Classifiers Against Adversarial Attacks Using TTTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description:\n",
    "\n",
    "##### This notebook leverages the TTTS (Tree Test Time Simulation) package to systematically assess the robustness of various Random Forest classifiers against adversarial attacks. The analysis is conducted across an extensive collection of 50 datasets, ensuring a broad and deep understanding of classifier performance under adversarial conditions. Key components of the notebook include:\n",
    "\n",
    "#### <ins>Dataset Preparation and Organization:</ins>\n",
    "\n",
    "##### - Initialization of base_path to set the directory for dataset storage.\n",
    "##### - Curation of dataset_list, bigger_datasets_list, and more_bigger_datasets_list to segregate datasets based on size for differentiated processing.\n",
    "\n",
    "#### <ins>Adversarial Robustness Assessment of Random Forest Classifiers:</ins>\n",
    "\n",
    "##### - Implementation of a robust testing framework using 5-fold StratifiedKFold cross-validation.\n",
    "##### - Configuration of various Random Forest classifiers, each with unique probability adjustment strategies.\n",
    "##### - Functionality for training classifiers, generating adversarial examples, applying defensive techniques, and evaluating multiple performance metrics.\n",
    "##### - Parallel compilation of evaluations in results_df_dtattack DataFrame, offering a structured analysis of each classifier's resilience against adversarial attacks.\n",
    "\n",
    "#### <ins>Evaluation Against Zoo Adversarial Attacks:</ins>\n",
    "\n",
    "##### - Detailed robustness analysis of Decision Tree classifiers, inclusive of a custom MonteCarlo RF implementation, when faced with Zoo adversarial attacks.\n",
    "##### - Parallel processing to efficiently train classifiers, generate adversarial samples, apply optional defenses, and compute key performance metrics.\n",
    "##### -Aggregation and visualization of results, providing insights into the defensive capabilities of each classifier against Zoo attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, log_loss, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer,fetch_lfw_pairs,load_digits,load_iris,load_wine\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from art.defences.preprocessor import FeatureSqueezing,GaussianAugmentation\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod,AutoProjectedGradientDescent,ThresholdAttack\n",
    "from art.attacks.evasion import ZooAttack,HopSkipJump, BoundaryAttack, DecisionTreeAttack\n",
    "from art.attacks.evasion import HighConfidenceLowUncertainty, ProjectedGradientDescent\n",
    "from sklearn.utils import Bunch\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from art.estimators.classification import SklearnClassifier\n",
    "from TTTS import MonteCarloRandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Loading and Processing of Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path for file storage and lists of dataset filenames\n",
    "# 'base_path' represents the directory where the dataset files are stored.\n",
    "# 'dataset_list' contains filenames of datasets to be processed.\n",
    "# 'bigger_datasets_list' and 'more_bigger_datasets_list' are subsets of larger datasets for additional processing options.\n",
    "\n",
    "# Base path where the files are stored\n",
    "base_path = \"../data/\"\n",
    "\n",
    "# List of files\n",
    "dataset_list = [\n",
    "    \"!ar4.csv\",\n",
    "    \"!bodyfat.csv\",\n",
    "    \"Kaggle_Surgical-deepnet.csv\",\n",
    "    \"MaternalBinary.csv\",\n",
    "    \"OPENML_philippine.csv\",\n",
    "    \"AcousticExtinguisherFire.csv\",\n",
    "    \"acute-inflammation.csv\",\n",
    "    \"acute-nephritis.csv\",\n",
    "    \"AP_Colon_Lung.csv\",\n",
    "    \"backache.csv\",\n",
    "    \"blood.csv\",\n",
    "    \"chess-krvkp.csv\",\n",
    "    \"cloud.csv\",\n",
    "    \"congressional-voting.csv\",\n",
    "    \"credit-approval.csv\",\n",
    "    \"dresses-salesN.csv\",\n",
    "    \"echocardiogram.csv\",\n",
    "    \"haberman-survival.csv\",\n",
    "    \"heart_failure_clinical_records_dataset.csv\",\n",
    "    \"heart-hungarian.csv\",\n",
    "    \"hill-valley.csv\",\n",
    "    \"horse-colic.csv\",\n",
    "    \"ilpd-indian-liver.csv\",\n",
    "    \"no2.csv\",\n",
    "    \"kaggle_REWEMA.csv\",\n",
    "    \"lowbwt.csv\",\n",
    "    \"madelon.csv\",\n",
    "    \"Mesothelioma.csv\",\n",
    "    \"MIMIC2.csv\",\n",
    "    \"molec-biol-promoter.csv\",\n",
    "    \"oil_spill.csv\",\n",
    "    \"oocytes_merluccius_nucleus_4d.csv\",\n",
    "    \"oocytes_trisopterus_nucleus_2f.csv\",\n",
    "    \"ozone.csv\",\n",
    "    \"Parkinson_Multiple_Sound_Recording.csv\",\n",
    "    \"PC1 Software defect prediction.csv\",\n",
    "    \"pd_speech_features.csv\",\n",
    "    \"pima.csv\",\n",
    "    \"Pistachio_28_Features_Dataset.csv\",\n",
    "    \"plasma_retinol.csv\",\n",
    "    \"primary-tumorNumeric.csv\",\n",
    "    \"seismic-bumps.csv\",\n",
    "    \"sleuth_case2002.csv\",\n",
    "    \"spambase.csv\",\n",
    "    \"spect.csv\",\n",
    "    \"spectf.csv\",\n",
    "    \"statlog-australian-credit.csv\",\n",
    "    \"statlog-heart_.csv\",\n",
    "    \"ThoraricSurgery.csv\",\n",
    "    \"triazines.csv\",\n",
    "]\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(file_name, path):\n",
    "    try:\n",
    "        data = pd.read_csv(path + file_name)\n",
    "        # Use all columns except the last one as features\n",
    "        X = data.iloc[:, :-1]\n",
    "        # Use the last column as the target class\n",
    "        y = data.iloc[:, -1]\n",
    "        return (file_name, Bunch(data=X, target=y))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Parallel loading of datasets (using all available cores with n_jobs=-1)\n",
    "datasets = Parallel(n_jobs=-1)(\n",
    "    delayed(load_dataset)(file_name, base_path) for file_name in dataset_list\n",
    ")\n",
    "\n",
    "# Filter out None values in case of loading errors\n",
    "datasets = [dataset for dataset in datasets if dataset is not None]\n",
    "\n",
    "# Now 'datasets' is a list of tuples, where each tuple contains file_name and the corresponding dataset as a Bunch object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Robustness Assessment of Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!ar4.csv\n",
      "!bodyfat.csv\n",
      "Kaggle_Surgical-deepnet.csv\n",
      "MaternalBinary.csv\n",
      "OPENML_philippine.csv\n",
      "AcousticExtinguisherFire.csv\n",
      "acute-inflammation.csv\n",
      "acute-nephritis.csv\n",
      "backache.csv\n",
      "blood.csv\n",
      "chess-krvkp.csv\n",
      "cloud.csv\n",
      "congressional-voting.csv\n",
      "credit-approval.csv\n",
      "dresses-salesN.csv\n",
      "echocardiogram.csv\n",
      "haberman-survival.csv\n",
      "heart_failure_clinical_records_dataset.csv\n",
      "heart-hungarian.csv\n",
      "hill-valley.csv\n",
      "horse-colic.csv\n",
      "ilpd-indian-liver.csv\n",
      "no2.csv\n",
      "kaggle_REWEMA.csv\n",
      "lowbwt.csv\n",
      "madelon.csv\n",
      "Mesothelioma.csv\n",
      "MIMIC2.csv\n",
      "molec-biol-promoter.csv\n",
      "oil_spill.csv\n",
      "oocytes_merluccius_nucleus_4d.csv\n",
      "oocytes_trisopterus_nucleus_2f.csv\n",
      "ozone.csv\n",
      "Parkinson_Multiple_Sound_Recording.csv\n",
      "PC1 Software defect prediction.csv\n",
      "pd_speech_features.csv\n",
      "pima.csv\n",
      "Pistachio_28_Features_Dataset.csv\n",
      "plasma_retinol.csv\n",
      "primary-tumorNumeric.csv\n",
      "seismic-bumps.csv\n",
      "sleuth_case2002.csv\n",
      "spambase.csv\n",
      "spect.csv\n",
      "spectf.csv\n",
      "statlog-australian-credit.csv\n",
      "statlog-heart_.csv\n",
      "ThoraricSurgery.csv\n",
      "triazines.csv\n"
     ]
    }
   ],
   "source": [
    "# This notebook cell conducts an adversarial robustness assessment of various Random Forest classifiers using 5-fold cross-validation. The process involves:\n",
    "\n",
    "# Initializing StratifiedKFold for equitable distribution across folds.\n",
    "# Defining a suite of Decision Tree classifiers, each tailored with unique probability adjustment strategies.\n",
    "# Implementing the evaluate_classifier function to train classifiers, generate adversarial examples, apply defensive techniques (if applicable), \n",
    "# and evaluate performance metrics such as AUC, F1 score, Log Loss, Accuracy, and Runtime.\n",
    "# Iterating over datasets and classifiers, the cell conducts cross-validation, compiles the evaluations in parallel, \n",
    "# and organizes the results in a DataFrame results_df_dtattack for a structured analysis of each classifier's resilience against adversarial attacks.\n",
    "\n",
    "# Initialize a 5-fold StratifiedKFold object for cross-validation, ensuring each fold is a good representative of the whole.\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize a list to store the evaluation results of the classifiers.\n",
    "results = []\n",
    "\n",
    "# Define a dictionary of classifiers to evaluate. Each key is the classifier name and the value is the classifier object.\n",
    "classifiers = {\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=123, n_estimators=10),\n",
    "    \"MonteCarloRandomForest_Fix_Prob\": MonteCarloRandomForestClassifier(\n",
    "        random_state=123, prob_type=\"fixed\", n_estimators=10\n",
    "    ),\n",
    "    \"MonteCarloRandomForest_Depth_Prob\": MonteCarloRandomForestClassifier(\n",
    "        random_state=123, prob_type=\"depth\", n_estimators=10\n",
    "    ),\n",
    "    \"MonteCarloRandomForest_Agreement_Prob\": MonteCarloRandomForestClassifier(\n",
    "        random_state=123, prob_type=\"agreement\", n_estimators=10\n",
    "    ),\n",
    "    \"MonteCarloRandomForest_Bayes_Prob\": MonteCarloRandomForestClassifier(\n",
    "        random_state=123, prob_type=\"bayes\", n_estimators=10\n",
    "    ),\n",
    "    \"MonteCarloRandomForest_Confidence_Prob\": MonteCarloRandomForestClassifier(\n",
    "        random_state=123, prob_type=\"confidence\", n_estimators=10\n",
    "    ),\n",
    "    \"MonteCarloRandomForest_Distance_Prob\": MonteCarloRandomForestClassifier(\n",
    "        random_state=123, prob_type=\"distance\", n_estimators=10\n",
    "    ),\n",
    "    \"FeatureSqueezing\": RandomForestClassifier(random_state=123, n_estimators=10),\n",
    "}\n",
    "\n",
    "\n",
    "# Define a function to evaluate a classifier. This function trains the classifier, makes predictions, and calculates evaluation metrics.\n",
    "def evaluate_classifier(dataset_name, dataset, clf_name, clf, train_index, test_index):\n",
    "    # Prepare the data by filling NA values with 0 and extracting features (X) and target variable (y).\n",
    "    X, y = dataset.data.fillna(0).values, dataset.target\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the classifier and measure the time taken for training.\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set.\n",
    "    pred_probs = clf.predict_proba(X_test)\n",
    "    preds = clf.predict(X_test)\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    # Calculate evaluation metrics: AUC, F1-score, Log Loss, Accuracy.\n",
    "    auc = roc_auc_score(y_test, pred_probs[:, 1], multi_class=\"ovr\")\n",
    "    f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    logloss = log_loss(y_test, pred_probs)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    # Return all calculated metrics.\n",
    "    return [dataset_name, clf_name, auc, f1, logloss, accuracy, runtime]\n",
    "\n",
    "\n",
    "# Prepare tasks for parallel execution. This involves creating a list of delayed task objects.\n",
    "all_tasks = []\n",
    "for dataset_name, dataset in datasets:\n",
    "    print(dataset_name)\n",
    "    X, y = dataset.data.fillna(0).values, dataset.target\n",
    "\n",
    "    # Create a task for each fold of each classifier for each dataset.\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "            task = delayed(evaluate_classifier)(\n",
    "                dataset_name, dataset, clf_name, clf, train_index, test_index\n",
    "            )\n",
    "            all_tasks.append(task)\n",
    "\n",
    "# Execute all tasks in parallel using joblib's Parallel. n_jobs=-1 means using all available CPU cores.\n",
    "results = Parallel(n_jobs=-1)(all_tasks)\n",
    "\n",
    "# Convert the list of results into a DataFrame for easier analysis and visualization. Each row contains the performance metrics of a classifier on a dataset.\n",
    "results_df_wo_rf = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Dataset\", \"Classifier\", \"AUC\", \"F1\", \"LogLoss\", \"Accuracy\", \"Runtime\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell aggregates the performance metrics for each classifier and dataset combination from results_df_dtattack, \n",
    "# calculating the mean and standard deviation of AUC, F1 score, Log Loss, Accuracy, and Runtime.\n",
    "\n",
    "summary_wo = results_df_wo_rf.groupby(['Dataset','Classifier']).agg({\n",
    "    'AUC': ['mean', 'std'],\n",
    "    'F1': ['mean', 'std'],\n",
    "    'LogLoss': ['mean', 'std'],\n",
    "    'Accuracy': ['mean', 'std'],\n",
    "    'Runtime': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "print(summary_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation of RF Classifiers and Monte Carlo RF Classifier under Zoo Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell evaluates the robustness of RF classifiers, including our MonteCarlo RF implementation, against Zoo adversarial attacks using 5-fold cross-validation. \n",
    "# It trains classifiers, generates adversarial samples, optionally applies defenses, and measures performance metrics. \n",
    "# The evaluation is executed in parallel for efficiency, and results are aggregated and displayed, providing insights into each classifier's defense against Zoo attacks.\n",
    "\n",
    "# 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store evaluation results\n",
    "results = []\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'RandomForest_ZooAttack':  RandomForestClassifier(random_state=123,n_estimators=10),\n",
    "    'MonteCarloRandomForest_Fix_Prob_ZooAttack':  MonteCarloRandomForestClassifier(random_state=123,prob_type='fixed',n_estimators=10),\n",
    "    'MonteCarloRandomForest_Depth_Prob_ZooAttack':  MonteCarloRandomForestClassifier(random_state=123,prob_type='depth',n_estimators=10),\n",
    "    'MonteCarloRandomForest_Agreement_Prob_ZooAttack':  MonteCarloRandomForestClassifier(random_state=123,prob_type='agreement',n_estimators=10),\n",
    "    'MonteCarloRandomForest_Confidence_Prob_ZooAttack':  MonteCarloRandomForestClassifier(random_state=123,prob_type='confidence',n_estimators=10),\n",
    "    'MonteCarloRandomForest_Distance_Prob_ZooAttack':  MonteCarloRandomForestClassifier(random_state=123,prob_type='distance',n_estimators=10),\n",
    "    'FeatureSqueezing_ZooAttack': RandomForestClassifier(random_state=123,n_estimators=10),\n",
    "}\n",
    "\n",
    "def evaluate_classifier(dataset_name, dataset, clf_name, clf, train_index, test_index):\n",
    "    X, y = dataset.data.fillna(0).values, dataset.target\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    if clf_name != 'RandomForest_ZooAttack':\n",
    "        dummy_clf = RandomForestClassifier(random_state=123,n_estimators=10)\n",
    "        dummy_clf.fit(X_train, y_train)\n",
    "        classifier = SklearnClassifier(model=dummy_clf, use_logits=True)\n",
    "    else:\n",
    "        classifier = SklearnClassifier(model=clf, use_logits=True)\n",
    "        \n",
    "    attack = ZooAttack(classifier=classifier, confidence=0.0, targeted=False, learning_rate=1e-1, max_iter=5,\n",
    "           binary_search_steps=5, initial_const=1e-3, abort_early=True, use_resize=False, \n",
    "           use_importance=False, nb_parallel=2, batch_size=1, variable_h=0.2) \n",
    "    x_test_adv = attack.generate(x=X_test)\n",
    "    \n",
    "    if clf_name == 'FeatureSqueezing_ZooAttack':\n",
    "        # Initialize the feature squeezing defence\n",
    "        defence = FeatureSqueezing(clip_values=(X_train.min(), X_train.max()), bit_depth=4)\n",
    "\n",
    "        # Fit the defence with training data\n",
    "        defence.fit(X_train)\n",
    "\n",
    "        # Apply the defence on testing data\n",
    "        x_test_adv = defence(x_test_adv)[0]\n",
    "        \n",
    "    if clf_name == 'GaussianAugmentation_ZooAttack':\n",
    "        # Initialize the Gaussian Augmentation defence\n",
    "        defence = GaussianAugmentation(sigma=1.0)\n",
    "\n",
    "        # Apply the Gaussian Augmentation on training data\n",
    "        X_train_augmented, y_train_augmented = defence(X_train, y_train)\n",
    "\n",
    "        # Retrain the model on the augmented data\n",
    "        clf.fit(X_train_augmented, y_train_augmented)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pred_probs = clf.predict_proba(x_test_adv)\n",
    "    preds = clf.predict(x_test_adv)\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    # Evaluate\n",
    "    auc = roc_auc_score(y_test, pred_probs[:, 1], multi_class='ovr')\n",
    "    f1 = f1_score(y_test, preds, average='macro')\n",
    "    logloss = log_loss(y_test, pred_probs)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    return [dataset_name, clf_name, auc, f1, logloss, accuracy, runtime]\n",
    "\n",
    "# Loop through datasets\n",
    "all_tasks = []\n",
    "\n",
    "for dataset_name, dataset in datasets:\n",
    "    print(dataset_name)\n",
    "    X, y = dataset.data.fillna(0).values, dataset.target\n",
    "    try:\n",
    "        # Perform 5-fold cross-validation\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "                task = delayed(evaluate_classifier)(dataset_name, dataset, clf_name, clf, train_index, test_index)\n",
    "                all_tasks.append(task)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {clf_name} on {dataset_name}: {e}\")\n",
    "\n",
    "# Execute all tasks in parallel\n",
    "try:\n",
    "    results = Parallel(n_jobs=-1)(all_tasks)\n",
    "except Exception as e:\n",
    "    print(f\"Error in {clf_name} on {dataset_name}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame for easy visualization\n",
    "results_df_ZooAttack = pd.DataFrame(results, columns=['Dataset', 'Classifier', 'AUC', 'F1', 'LogLoss', 'Accuracy', 'Runtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ZooAttack = results_df_ZooAttack.groupby(['Dataset','Classifier']).agg({\n",
    "    'AUC': ['mean', 'std'],\n",
    "    'F1': ['mean', 'std'],\n",
    "    'LogLoss': ['mean', 'std'],\n",
    "    'Accuracy': ['mean', 'std'],\n",
    "    'Runtime': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "print(summary_ZooAttack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([summary_wo, summary_ZooAttack]).sort_values(by='Dataset').to_csv('50ds_rf_summary.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (bgu)",
   "language": "python",
   "name": "bgu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "82595bd2b2069ccb6d1ac59e6bd6e9eaab28e59d5748ed3226bea45e61f239f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
